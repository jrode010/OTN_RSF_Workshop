[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Relative habitat selection and resource selection functions in aquatic acoustic telemetry: theory, application, and process",
    "section": "",
    "text": "Lucas Griffin (University of South Florida, USA)\nJonathan Rodemann (Florida International University, USA)\n\n\n\nRobert Lennox (OTN, Canada)\n\n\n\nWelcome to our Resource Selection Function workshop! The goal of this workshop is to familiarize participants with the concept of RSFs, present case studies within aquatic acoustic telemetry, and how to run the analyses in R. We will start the workshop by providing background and theory on RSFs. We will then present multiple case studies using RSFs from our and other’s work in aquatic telemetry systems. These case studies will be followed a walkthrough of the data and R code used to conduct RSFs using Random Forest, Generalized Linear, and Generalized Additive Models. R scripts are provided to encourage participants to conduct Resource Selection Functions on their own acoustic data.\n\nResource selection functions (RSFs), defined as a function that produces values that are proportional to the probability of use by an animal, are a popular method to determine and predict relative habitat selection by animals. These functions evaluate the relationships between resource use (i.e., the units of area selected by an animal) and the environmental characteristics associated with each unit of area. Animal spatial data, from sources such as telemetry, can be incorporated into RSFs to define the relative habitat selection strengths among animal space use and a given set of environmental covariates, such as habitat type, substrate, elevation, or water depth. When the true absences are unknown, as generated by presence only data derived from sources such as telemetry approaches, RSFs are implemented within a use/availability framework where known presences (1) are compared with a random sample across ‘available’ resource units, also known as pseudo-absences or background points (0). Alternative to use/availability (e.g., from telemetry), data from observations collected from survey methods, often without timestamps, are typically referred to as presence-background and are fitted as species distribution models. Using RSFs to derive the relative probability of selection, rather than the absolute probability, telemetry data are then typically fitted using logistic regression models or, as of more recently, with machine learning algorithms [e.g., random forest (RF), boosted regression trees].\n\n\n\n\nWorkshop GitHub – Website that houses data and code for our RSF workshop\n\nWorkshop slides\n\n\nManly et al. 2007. Resource Selection by Animals: Statistical Design and Analysis for Field Studies\n\nBoyce et al. 2002. Evaluating resource selection functions. Ecol. Model.\n\nBoyce 2006. Scale for resource selection functions. Drivers. Distrib.\n\nBoyce and McDonald 1999. Relating populations to habitats using resource selection functions. Trends Ecol. Evol.\n\n\n\nSea turtle: Selby et al., 2019. Juvenile hawksbill residency and habitat use within a Caribbean marine protected area. End. Species Res.\nSharks: Griffin et al., 2021. A novel framework to predict relative habitat selection in aquatic systems: Applying machine learning and resource selection functions to acoustic telemetry data from multiple shark species. Front. Mar. Sci.\nSeatrout: Rodeman et al. 2024. Finding a home in a fragmented world: Multi-scale habitat selection of Spotted Seatrout in an area of seagrass recovery. in prep.\n\n\n\n\nBrownscombe et al., 2022. Applications of telemetry to fish habitat science and management. Canadian Journal of Fisheries and Aquatic Sciences.\n\nBangley et al., 2022. Modeling the probability of overlap between marine fish distributions and marine renewable energy infrastructure using acoustic telemetry data. Front. Mar. Sci.\nLandovskis et al., 2024. Habitat and movement selection processes of American lobster within a restricted bay in the Bras d’Or Lake, Nova Scotia, Canada. Movement Ecology.\nKressler et al., 2024. Habitat selection and spatial behavior of vulnerable juvenile lemon sharks: Implications for conservation. Ecological Indicators.\nvan Zinnicq Bergmann et al., 2024. Intraguild processes drive space-use patterns in a large-bodied marine predator community. Journal of Animal Ecology."
  },
  {
    "objectID": "index.html#main-conveners",
    "href": "index.html#main-conveners",
    "title": "Relative habitat selection and resource selection functions in aquatic acoustic telemetry: theory, application, and process",
    "section": "",
    "text": "Lucas Griffin (University of South Florida, USA)\nJonathan Rodemann (Florida International University, USA)"
  },
  {
    "objectID": "index.html#co-conveners",
    "href": "index.html#co-conveners",
    "title": "Relative habitat selection and resource selection functions in aquatic acoustic telemetry: theory, application, and process",
    "section": "",
    "text": "Robert Lennox (OTN, Canada)"
  },
  {
    "objectID": "index.html#about-this-workshop",
    "href": "index.html#about-this-workshop",
    "title": "Relative habitat selection and resource selection functions in aquatic acoustic telemetry: theory, application, and process",
    "section": "",
    "text": "Welcome to our Resource Selection Function workshop! The goal of this workshop is to familiarize participants with the concept of RSFs, present case studies within aquatic acoustic telemetry, and how to run the analyses in R. We will start the workshop by providing background and theory on RSFs. We will then present multiple case studies using RSFs from our and other’s work in aquatic telemetry systems. These case studies will be followed a walkthrough of the data and R code used to conduct RSFs using Random Forest, Generalized Linear, and Generalized Additive Models. R scripts are provided to encourage participants to conduct Resource Selection Functions on their own acoustic data.\n\nResource selection functions (RSFs), defined as a function that produces values that are proportional to the probability of use by an animal, are a popular method to determine and predict relative habitat selection by animals. These functions evaluate the relationships between resource use (i.e., the units of area selected by an animal) and the environmental characteristics associated with each unit of area. Animal spatial data, from sources such as telemetry, can be incorporated into RSFs to define the relative habitat selection strengths among animal space use and a given set of environmental covariates, such as habitat type, substrate, elevation, or water depth. When the true absences are unknown, as generated by presence only data derived from sources such as telemetry approaches, RSFs are implemented within a use/availability framework where known presences (1) are compared with a random sample across ‘available’ resource units, also known as pseudo-absences or background points (0). Alternative to use/availability (e.g., from telemetry), data from observations collected from survey methods, often without timestamps, are typically referred to as presence-background and are fitted as species distribution models. Using RSFs to derive the relative probability of selection, rather than the absolute probability, telemetry data are then typically fitted using logistic regression models or, as of more recently, with machine learning algorithms [e.g., random forest (RF), boosted regression trees]."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Relative habitat selection and resource selection functions in aquatic acoustic telemetry: theory, application, and process",
    "section": "",
    "text": "Workshop GitHub – Website that houses data and code for our RSF workshop\n\nWorkshop slides\n\n\nManly et al. 2007. Resource Selection by Animals: Statistical Design and Analysis for Field Studies\n\nBoyce et al. 2002. Evaluating resource selection functions. Ecol. Model.\n\nBoyce 2006. Scale for resource selection functions. Drivers. Distrib.\n\nBoyce and McDonald 1999. Relating populations to habitats using resource selection functions. Trends Ecol. Evol.\n\n\n\nSea turtle: Selby et al., 2019. Juvenile hawksbill residency and habitat use within a Caribbean marine protected area. End. Species Res.\nSharks: Griffin et al., 2021. A novel framework to predict relative habitat selection in aquatic systems: Applying machine learning and resource selection functions to acoustic telemetry data from multiple shark species. Front. Mar. Sci.\nSeatrout: Rodeman et al. 2024. Finding a home in a fragmented world: Multi-scale habitat selection of Spotted Seatrout in an area of seagrass recovery. in prep.\n\n\n\n\nBrownscombe et al., 2022. Applications of telemetry to fish habitat science and management. Canadian Journal of Fisheries and Aquatic Sciences.\n\nBangley et al., 2022. Modeling the probability of overlap between marine fish distributions and marine renewable energy infrastructure using acoustic telemetry data. Front. Mar. Sci.\nLandovskis et al., 2024. Habitat and movement selection processes of American lobster within a restricted bay in the Bras d’Or Lake, Nova Scotia, Canada. Movement Ecology.\nKressler et al., 2024. Habitat selection and spatial behavior of vulnerable juvenile lemon sharks: Implications for conservation. Ecological Indicators.\nvan Zinnicq Bergmann et al., 2024. Intraguild processes drive space-use patterns in a large-bodied marine predator community. Journal of Animal Ecology."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Rinst.html",
    "href": "Rinst.html",
    "title": "OTN Symposium 2024 RSF Workshop",
    "section": "",
    "text": "R and RStudio\nR and RStudio are separate downloads and installations. R is the underlying statistical computing environment, but using R alone is no fun. RStudio is a graphical integrated development environment (IDE) that makes using R much easier and more interactive. You need to install R before you install RStudio. In the sections below are the instructions for installing R and R Studio on your operating system.\n\nWindows\n\nIf you already have R and RStudio installed\n\nOpen RStudio, and click on “Help” &gt; “Check for updates”. If a new version is available, quit RStudio, and download the latest version for RStudio.\nTo check which version of R you are using, start RStudio and the first thing that appears in the console indicates the version of R you are running. Alternatively, you can type sessionInfo(), which will also display which version of R you are running. Go on the CRAN website and check whether a more recent version is available. If so, please download and install it. You can check here for more information on how to remove old versions from your system if you wish to do so.\n\n\n\nIf you don’t have R and RStudio installed\n\nDownload R from the CRAN website.\nRun the .exe file that was just downloaded\nGo to the RStudio download page\nUnder Installers select RStudio x.yy.zzz - Windows 10/11 (where x, y, and z represent version numbers)\nDouble click the file to install it\nOnce it’s installed, open RStudio to make sure it works and you don’t get any error messages.\n\n\n\n\nmacOS\n\nIf you already have R and RStudio installed\n\nOpen RStudio, and click on “Help” &gt; “Check for updates”. If a new version is available, quit RStudio, and download the latest version for RStudio.\nTo check the version of R you are using, start RStudio and the first thing that appears on the terminal indicates the version of R you are running. Alternatively, you can type sessionInfo(), which will also display which version of R you are running. Go on the CRAN website and check whether a more recent version is available. If so, please download and install it.\n\n\n\nIf you don’t have R and RStudio installed\n\nDownload R from the CRAN website.\nSelect the .pkg file for the latest R version\nDouble click on the downloaded file to install R\nIt is also a good idea to install XQuartz (needed by some packages)\nGo to the RStudio download page\nUnder Installers select RStudio x.yy.zzz - Mac OS X 10.15+ (64-bit) (where x, y, and z represent version numbers)\nDouble click the file to install RStudio\nOnce it’s installed, open RStudio to make sure it works and you don’t get any error messages.\n\n\n\n\nLinux\n\nFollow the instructions for your distribution from CRAN, they provide information to get the most recent version of R for common distributions. For most distributions, you could use your package manager (e.g., for Debian/Ubuntu run sudo apt-get install r-base, and for Fedora sudo yum install R), but we don’t recommend this approach as the versions provided by this are usually out of date. In any case, make sure you have at least R 3.3.1.\nGo to the RStudio download page\nUnder Installers select the version that matches your distribution, and install it with your preferred method (e.g., with Debian/Ubuntu sudo dpkg -i   rstudio-x.yy.zzz-amd64.deb at the terminal).\nOnce it’s installed, open RStudio to make sure it works and you don’t get any error messages."
  },
  {
    "objectID": "RF.html",
    "href": "RF.html",
    "title": "RSF with Random Forest Models",
    "section": "",
    "text": "This vignette uses Random Forest to calculate Resource Selection Functions. We will step you through the steps in order to set up the RSFs and then run an example from Spotted Seatrout in Florida Bay (Rodemann et al. 2024 in prep)\nR script"
  },
  {
    "objectID": "RF.html#rsfs-with-random-forest-modelling",
    "href": "RF.html#rsfs-with-random-forest-modelling",
    "title": "RSF with Random Forest Models",
    "section": "",
    "text": "This vignette uses Random Forest to calculate Resource Selection Functions. We will step you through the steps in order to set up the RSFs and then run an example from Spotted Seatrout in Florida Bay (Rodemann et al. 2024 in prep)\nR script"
  },
  {
    "objectID": "RF.html#data",
    "href": "RF.html#data",
    "title": "RSF with Random Forest Models",
    "section": "data",
    "text": "data\nThe data used for this example was collected by Rodemann et al. in Florida Bay. Acoustic telemetry data is from tagged spotted seatrout and environmental data is from field surveys done by Rodemann and team.\n\n\n\nMap of sampling basins\n\n\n\n# load libraries\nlibrary(terra, exclude = 'resample') #work with spatial data - rasters\n## terra 1.7.71\nlibrary(raster) #work with spatial data - rasters\n## Loading required package: sp\nlibrary(sf) #Work with spatial data - shapefiles\n## Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\nlibrary(sfheaders) #work with spatial data\nlibrary(chron) #visualization of acoustic data\n## \n## Attaching package: 'chron'\n## The following objects are masked from 'package:raster':\n## \n##     origin, origin&lt;-\n## The following objects are masked from 'package:terra':\n## \n##     origin, origin&lt;-\nlibrary(splitstackshape) #break up data into smaller columns\nlibrary(scales)#visualization of acoustic data\n## \n## Attaching package: 'scales'\n## The following object is masked from 'package:terra':\n## \n##     rescale\nlibrary(mlr3verse) # Machine learning\n## Loading required package: mlr3\n## \n## Attaching package: 'mlr3'\n## The following object is masked from 'package:raster':\n## \n##     resample\nlibrary(mlr3spatial) #spatial machine learning\nlibrary(randomForest) #Machine learning\n## randomForest 4.7-1.1\n## Type rfNews() to see new features/changes/bug fixes.\nlibrary(iml) #result interpretation\nlibrary(ranger) #Machine learning\n## \n## Attaching package: 'ranger'\n## The following object is masked from 'package:randomForest':\n## \n##     importance\nlibrary(tidyverse) #organize and visualize data\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.2\n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ readr::col_factor()  masks scales::col_factor()\n## ✖ dplyr::combine()     masks randomForest::combine()\n## ✖ lubridate::days()    masks chron::days()\n## ✖ purrr::discard()     masks scales::discard()\n## ✖ tidyr::extract()     masks raster::extract(), terra::extract()\n## ✖ dplyr::filter()      masks stats::filter()\n## ✖ lubridate::hours()   masks chron::hours()\n## ✖ dplyr::lag()         masks stats::lag()\n## ✖ ggplot2::margin()    masks randomForest::margin()\n## ✖ lubridate::minutes() masks chron::minutes()\n## ✖ lubridate::origin()  masks chron::origin(), raster::origin(), terra::origin()\n## ✖ lubridate::seconds() masks chron::seconds()\n## ✖ dplyr::select()      masks raster::select()\n## ✖ lubridate::years()   masks chron::years()\n## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nlibrary(ggmap) #plotting onto map\n## ℹ Google's Terms of Service: &lt;https://mapsplatform.google.com&gt;\n##   Stadia Maps' Terms of Service: &lt;https://stadiamaps.com/terms-of-service/&gt;\n##   OpenStreetMap's Tile Usage Policy: &lt;https://operations.osmfoundation.org/policies/tiles/&gt;\n## ℹ Please cite ggmap if you use it! Use `citation(\"ggmap\")` for details.\n## \n## Attaching package: 'ggmap'\n## \n## \n## The following object is masked from 'package:terra':\n## \n##     inset\nlibrary(beepr) #beeps when code is done running\n\n# load sav monitoring data \ndt &lt;- read.csv('data/Acoustic_data.csv') #Acoustic data\ntags &lt;- read.csv('data/Tag_Metadata.csv') #tag metadata\nstations &lt;- read.csv('data/Stations.csv')\nhead(dt)\n##   X Date.and.Time..UTC.    Receiver    Transmitter Transmitter.Name\n## 1 1 2020-03-01 16:50:33 VR2W-128959 A69-1602-37774               NA\n## 2 2 2020-03-01 16:52:06 VR2W-128959 A69-1602-37774               NA\n## 3 3 2020-03-01 17:08:34 VR2W-128959 A69-1602-37774               NA\n## 4 4 2020-03-01 17:10:55 VR2W-128959 A69-1602-37773               NA\n## 5 5 2020-03-01 17:13:35 VR2W-128959 A69-1602-37773               NA\n## 6 6 2020-03-01 17:15:02 VR2W-128959 A69-1602-37773               NA\n##   Transmitter.Serial Sensor.Value Sensor.Unit Station.Name Latitude Longitude\n## 1                 NA           NA          NA          RB9       NA        NA\n## 2                 NA           NA          NA          RB9       NA        NA\n## 3                 NA           NA          NA          RB9       NA        NA\n## 4                 NA           NA          NA          RB9       NA        NA\n## 5                 NA           NA          NA          RB9       NA        NA\n## 6                 NA           NA          NA          RB9       NA        NA\n##   Transmitter.Type Sensor.Precision\n## 1               NA               NA\n## 2               NA               NA\n## 3               NA               NA\n## 4               NA               NA\n## 5               NA               NA\n## 6               NA               NA"
  },
  {
    "objectID": "RF.html#prepare-data",
    "href": "RF.html#prepare-data",
    "title": "RSF with Random Forest Models",
    "section": "Prepare data",
    "text": "Prepare data\nFirst we need to set up the acoustic data in order to work with it. This includes changing the time column into an actual time column and merging it with the metadata to add locations. Then we can plot the detection history to visualize when and where our fish were picked up\n\ndt &lt;- dt %&gt;% dplyr::select(-c(Transmitter.Name, Transmitter.Serial, Sensor.Value, Sensor.Unit, Latitude, Longitude, Transmitter.Type, Sensor.Precision))\n\n#change Date_time into posixct\ndt$Date_time &lt;- as.POSIXct(dt$Date.and.Time..UTC., format='%Y-%m-%d %H:%M:%S', tz='UTC')\n\n#abacus plot of detections\ntrout_det &lt;- ggplot(dt, aes(Date_time, Transmitter, col = Receiver)) + \n  geom_point() +\n  scale_x_datetime(labels = date_format(\"%Y-%m-%d\"),\n                   date_breaks = \"3 months\", limits = as.POSIXct(c('2020-01-27 00:00:00', '2021-01-01 00:00:00')))+\n  labs(x = \"Date\",  y = \"Transmitter\") +\n  # Change the angle and text size of x-axis tick labels to make more readable in final plots\n  theme_bw()+\n  theme(axis.text.x=element_text(angle= 50, size=10, vjust = 1, hjust=1),panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(axis.text.y=element_text(size=8))+\n  labs(title = \"Trout Detections\")\ntrout_det\n\n\n\n\n\n\n\n\ndat &lt;- merge(dt, stations, by=\"Station.Name\")\ndat &lt;- dat %&gt;% dplyr::select(-c(X, Receiver.x, Receiver.y, SAV))\nstr(dat)\n## 'data.frame':    37807 obs. of  7 variables:\n##  $ Station.Name       : chr  \"RB1\" \"RB1\" \"RB1\" \"RB1\" ...\n##  $ Date.and.Time..UTC.: chr  \"2020-11-15 16:24:57\" \"2020-12-13 13:04:35\" \"2020-10-03 05:23:22\" \"2020-10-21 00:10:56\" ...\n##  $ Transmitter        : chr  \"A69-1602-37798\" \"A69-1602-37787\" \"A69-1602-37771\" \"A69-1602-37787\" ...\n##  $ Date_time          : POSIXct, format: \"2020-11-15 16:24:57\" \"2020-12-13 13:04:35\" ...\n##  $ Date_Established   : chr  \"2/26/2020\" \"2/26/2020\" \"2/26/2020\" \"2/26/2020\" ...\n##  $ Latitude           : num  25.1 25.1 25.1 25.1 25.1 ...\n##  $ Longitude          : num  -80.8 -80.8 -80.8 -80.8 -80.8 ..."
  },
  {
    "objectID": "RF.html#calculate-coas",
    "href": "RF.html#calculate-coas",
    "title": "RSF with Random Forest Models",
    "section": "Calculate COAs",
    "text": "Calculate COAs\nTo reduce spatial and temporal autocorrelation as well as shift our detection points away from only the receivers, we calculate Centers-of-Activity (COAs). These are the average position of your organism across a certain time step. This time step depends on your organisms mobility and activity level as well as the positions of your receivers. Then we can visualize where in the world we are and our positions\n\n#Ok, we have the acoustic data merged with station data so we have locations. Let's calculate Center of Activities (COAs)\n#split up data into time chunks - want the right balance of time based on your tag timing to reduce autocorrelation but also create enough data\nex &lt;- seq(from = trunc(min(dat$Date_time, na.rm = TRUE), \"day\"), \n          to = trunc(max(dat$Date_time, na.rm = TRUE), \"day\") + \n            86400, by = 3600) #We split this data up into 1 hour time bins\ndat$DateTime &lt;- cut(dat$Date_time, breaks = ex) #cut up the data to calculate COAs\nstr(dat)\n## 'data.frame':    37807 obs. of  8 variables:\n##  $ Station.Name       : chr  \"RB1\" \"RB1\" \"RB1\" \"RB1\" ...\n##  $ Date.and.Time..UTC.: chr  \"2020-11-15 16:24:57\" \"2020-12-13 13:04:35\" \"2020-10-03 05:23:22\" \"2020-10-21 00:10:56\" ...\n##  $ Transmitter        : chr  \"A69-1602-37798\" \"A69-1602-37787\" \"A69-1602-37771\" \"A69-1602-37787\" ...\n##  $ Date_time          : POSIXct, format: \"2020-11-15 16:24:57\" \"2020-12-13 13:04:35\" ...\n##  $ Date_Established   : chr  \"2/26/2020\" \"2/26/2020\" \"2/26/2020\" \"2/26/2020\" ...\n##  $ Latitude           : num  25.1 25.1 25.1 25.1 25.1 ...\n##  $ Longitude          : num  -80.8 -80.8 -80.8 -80.8 -80.8 ...\n##  $ DateTime           : Factor w/ 7368 levels \"2020-03-01\",\"2020-03-01 01:00:00\",..: 6233 6902 5190 5617 6903 5616 6902 4466 5615 6902 ...\n\n#Calculation of COAs\nset.seed(19)\ncoadat &lt;- dat %&gt;% dplyr::group_by(DateTime, Transmitter) %&gt;% dplyr::mutate(n = n()) %&gt;% dplyr::filter(n &gt;= 5) %&gt;% #Take out any time bin/fish combination with less than 5 detections (COAs will be comprised of at least 5 detections)\n  dplyr::group_by(DateTime, Transmitter) %&gt;% mutate(lat.coa = mean(Latitude), long.coa = mean(Longitude)) %&gt;% #calculate COAs\n  dplyr::select(-c(Date_time, Date.and.Time..UTC., Latitude, Longitude, Station.Name, Date_Established)) %&gt;% distinct() #remove uneeded columns and take out repeated columns\nhead(coadat)\n## # A tibble: 6 × 5\n## # Groups:   DateTime, Transmitter [6]\n##   Transmitter    DateTime                n lat.coa long.coa\n##   &lt;chr&gt;          &lt;fct&gt;               &lt;int&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n## 1 A69-1602-37787 2020-12-13 13:00:00    11    25.1    -80.8\n## 2 A69-1602-37771 2020-10-03 05:00:00    15    25.1    -80.8\n## 3 A69-1602-37787 2020-10-20 23:00:00    14    25.1    -80.8\n## 4 A69-1602-37787 2020-10-20 22:00:00    19    25.1    -80.8\n## 5 A69-1602-37783 2020-08-12 06:00:00     6    25.1    -80.8\n## 6 A69-1602-37798 2020-11-15 15:00:00    12    25.1    -80.8\n\n#So our COA lat and long are in the dataframe. Lets now take out fish with less than 50 COAs\ncoadat1 &lt;- coadat %&gt;% as.data.frame() %&gt;% dplyr::group_by(Transmitter) %&gt;%\n  dplyr::mutate(count = n()) %&gt;% dplyr::filter(count &gt;= 50) %&gt;% dplyr::select(-c(n, count)) %&gt;% dplyr::distinct()\nstr(coadat1)\n## gropd_df [2,113 × 4] (S3: grouped_df/tbl_df/tbl/data.frame)\n##  $ Transmitter: chr [1:2113] \"A69-1602-37787\" \"A69-1602-37771\" \"A69-1602-37787\" \"A69-1602-37787\" ...\n##  $ DateTime   : Factor w/ 7368 levels \"2020-03-01\",\"2020-03-01 01:00:00\",..: 6902 5190 5616 5615 3943 6232 3951 7008 5395 5370 ...\n##  $ lat.coa    : num [1:2113] 25.1 25.1 25.1 25.1 25.1 ...\n##  $ long.coa   : num [1:2113] -80.8 -80.8 -80.8 -80.8 -80.8 ...\n##  - attr(*, \"groups\")= tibble [11 × 2] (S3: tbl_df/tbl/data.frame)\n##   ..$ Transmitter: chr [1:11] \"A69-1602-37380\" \"A69-1602-37771\" \"A69-1602-37773\" \"A69-1602-37774\" ...\n##   ..$ .rows      : list&lt;int&gt; [1:11] \n##   .. ..$ : int [1:458] 34 71 79 142 160 174 186 198 204 239 ...\n##   .. ..$ : int [1:70] 2 16 17 20 29 31 38 46 53 58 ...\n##   .. ..$ : int [1:191] 349 350 354 369 387 396 435 448 449 457 ...\n##   .. ..$ : int [1:154] 326 347 382 385 395 399 428 439 455 474 ...\n##   .. ..$ : int [1:232] 154 231 242 256 268 273 280 281 285 315 ...\n##   .. ..$ : int [1:209] 26 27 35 40 42 49 50 51 55 57 ...\n##   .. ..$ : int [1:94] 5 7 18 85 114 181 212 219 243 298 ...\n##   .. ..$ : int [1:166] 1 3 4 8 13 56 108 109 128 130 ...\n##   .. ..$ : int [1:62] 32 36 101 115 117 129 131 156 162 169 ...\n##   .. ..$ : int [1:356] 9 10 11 19 21 22 28 30 33 37 ...\n##   .. ..$ : int [1:121] 6 12 14 15 23 24 25 139 193 264 ...\n##   .. ..@ ptype: int(0) \n##   ..- attr(*, \".drop\")= logi TRUE\n\n\n#First, let's plot this in ggmap to get our bearings LG\n# Note,a API key from Google Maps is required to load the map and run the below code. Visit https://rpubs.com/ktcross/1154003 for instructions on  how to install and run ggamps in R\n# For now, we will use the already saved maps but the code to generate the maps if you have an existing API code is below as well.\n\n# FLmap_zoom_out &lt;- get_googlemap(center = c(lon=mean(coadat1$long.coa), lat=mean(coadat1$lat.coa)),\n#                        zoom = 6,\n#                        maptype = c(\"satellite\"))\nload(\"data/FLmap_zoom_out.RData\")\n\nggmap(FLmap_zoom_out, extent='normal') + \n  geom_point(data = coadat1,\n             aes(x = long.coa, y = lat.coa,), col =\"yellow\") \n\n\n\n\n\n\n\n\n# FLmap_zoom_in &lt;- get_googlemap(center = c(lon=mean(coadat1$long.coa), lat=mean(coadat1$lat.coa)),\n#                                 zoom = 13,\n#                                 maptype = c(\"satellite\"))\nload(\"data/FLmap_zoom_in.RData\")\n\nggmap(FLmap_zoom_in, extent='normal') + \n  geom_point(data = coadat1,\n             aes(x = long.coa, y = lat.coa,), col =\"yellow\") +\n  facet_wrap(~Transmitter)\n\n\n\n\n\n\n\n\n#now we have a dataframe with our transmitter, datetime bin, and coa lat and long. Let's look at the COAs on a map\n#create a spatial object in sf\nsfdat &lt;- coadat1 %&gt;% \n  st_as_sf(coords = c('long.coa', 'lat.coa')) %&gt;% #set up the coordinates\n  st_set_crs(4326) %&gt;%  # using 4326 for lat/lon decimal\n  st_transform(2958) #Transform data into projected coordinate system\n\n#We can now graph this in ggplot to confirm\nggplot() +\n  geom_sf(data = sfdat, size = 3)\n\n\n\n\n\n\n\n\n#this is now projected data, so revert it back!\ncoor &lt;- as.data.frame(do.call('rbind', sfdat$geometry)) %&gt;% rename(x = V1, y = V2)\n\ncoadat1 &lt;- cbind(coadat1, coor) %&gt;% dplyr::select(-c(lat.coa, long.coa))\n\n#As you can see, the data is limited to our grid of receivers. This is a downside to acoustic telemetry vs positioning solvers"
  },
  {
    "objectID": "RF.html#calculating-pseudo-absence-points",
    "href": "RF.html#calculating-pseudo-absence-points",
    "title": "RSF with Random Forest Models",
    "section": "Calculating pseudo-absence points",
    "text": "Calculating pseudo-absence points\nBecause acoustic telemetry data does not inherently have absence points (it only provides presence points), we need to calculate what are called “pseudo-absence” points. These are randomly placed points across your area of study for every presence point you have. How many pseudo-absence points depeneds on what models you are using. For Random Forest, a 1 to 1 ratio is best.\nFor this, it is best to break up the points into temporal categories to increase the robustness of the model. We are doing diel period (night/day) and season.\n\n#So we have the the real detections. Now we need to create pseudo-absences to compare the presences to.\n#For Random Forest, we do a 1 to 1 ratio of presence and pseudo-absence points. Barbet-Massin et al., 2012 Meth Ecol Evol\nset.seed(19) #repeatability of randomness\n\n#we are going to break up the data by individual, diel period, and season. This is for GLMM and GAM calcs, not for RF\n#set up data for break-up\nhead(coadat1)\n## # A tibble: 6 × 4\n## # Groups:   Transmitter [4]\n##   Transmitter    DateTime                  x        y\n##   &lt;chr&gt;          &lt;fct&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n## 1 A69-1602-37787 2020-12-13 13:00:00 518197. 2779938.\n## 2 A69-1602-37771 2020-10-03 05:00:00 518203. 2780157.\n## 3 A69-1602-37787 2020-10-20 23:00:00 518203. 2780157.\n## 4 A69-1602-37787 2020-10-20 22:00:00 518203. 2780157.\n## 5 A69-1602-37783 2020-08-12 06:00:00 518203. 2780157.\n## 6 A69-1602-37798 2020-11-15 15:00:00 518203. 2780157.\n\ncoadat1$DateTime2 &lt;- coadat1$DateTime\n\ncoadat1 &lt;- cSplit(coadat1, \"DateTime2\", sep = \" \", type.convert = F)\n\n\n# Rename columns so they make sense, it'll just be your last 2 column numbers, in this case the 15th and 16th column s\ncolnames(coadat1)[5:6]&lt;-c(\"Date\", \"Time\")\n\n\n# Then I repeat this and parse the date into year, month, and day (then hour, minute, second), so I can easily subset by year\n# Copy date to parse out, maintain original date\ncoadat1$Date2 &lt;- coadat1$Date\n\n\ncoadat1&lt;-cSplit(coadat1, \"Date2\", sep = \"-\", type.convert = FALSE)\ncolnames(coadat1)[7:9]&lt;-c(\"Year\", \"Month\", \"Day\")\n\ncoadat1$Time2 &lt;- coadat1$Time\n\ncoadat1&lt;-cSplit(coadat1, \"Time2\", sep = \":\", type.convert = FALSE)\ncolnames(coadat1)[10:12]&lt;-c(\"Hour\", \"Minute\", \"Second\")\n\ncoadat1$Date &lt;- as.Date(coadat1$Date)\ncoadat1$Time &lt;- as.times(coadat1$Time)\ncoadat1$Year &lt;- as.numeric(coadat1$Year)\ncoadat1$Month &lt;- as.numeric(coadat1$Month)\ncoadat1$Day &lt;- as.numeric(coadat1$Day)\ncoadat1$Hour &lt;- as.numeric(coadat1$Hour)\ncoadat1$Minute &lt;- as.numeric(coadat1$Minute)\ncoadat1$Second &lt;- as.numeric(coadat1$Second)\n\ncoadat1[is.na(coadat1)] &lt;- 0\n\n#diel period\nfor (i in 1:nrow(coadat1)){\n  if (coadat1$Hour[i] &gt;= 0 & coadat1$Hour[i] &lt; 6){\n    coadat1$period[i] &lt;- 'Night'\n  }else if (coadat1$Hour[i] &gt;= 6 & coadat1$Hour[i] &lt; 12){\n    coadat1$period[i] &lt;- 'Dawn'\n  }else if (coadat1$Hour[i] &gt;= 12 & coadat1$Hour[i] &lt; 18){\n    coadat1$period[i] &lt;- 'Day'\n  }else if (coadat1$Hour[i] &gt;= 18 & coadat1$Hour[i] &lt;= 24){\n    coadat1$period[i] &lt;- 'Dusk'\n  }\n}\n\n\n#season\nfor (i in 1:nrow(coadat1)){\n  if (coadat1$Month[i] &gt;= 8 && coadat1$Month[i] &lt;= 10){\n    coadat1$periody[i] &lt;- 'ew'\n  }else if (coadat1$Month[i] &gt;= 2 && coadat1$Month[i] &lt;= 4){\n    coadat1$periody[i] &lt;- 'ed'\n  }else if (coadat1$Month[i] &gt;= 5 && coadat1$Month[i] &lt;= 7){\n    coadat1$periody[i] &lt;- 'd'\n  }else{\n    coadat1$periody[i] &lt;- 'w'\n  }\n}\n\n#create list with number of occurences with each combination\nCOA_list &lt;- coadat1 %&gt;% as.data.frame() %&gt;%\n  group_by(Transmitter, period, periody) %&gt;%\n  # Calculate the number of occurrences with this transmitter, year, diel combination.\n  dplyr::summarise(count = n()) %&gt;% \n  ungroup() %&gt;%\n  # Combine the columns as unique ID.\n  mutate(TYD = paste0(Transmitter, \"_\", period, \"_\", periody)) %&gt;% \n  filter(count &gt;= 5) %&gt;% #keep combinations with only 5 or above observations\n  # Select only 2013.\n  #filter(Year == 2013) %&gt;% \n  # Make into list based on TYD.\n  group_split(TYD) \n## `summarise()` has grouped output by 'Transmitter', 'period'. You can override\n## using the `.groups` argument.\n\nextent &lt;- st_read('data/trainr2021_mask.shp')\n## Reading layer `trainr2021_mask' from data source \n##   `C:\\Users\\jonro\\OneDrive\\Desktop\\RSF_OTN_Workshop\\RSF_OTN_Workshop\\data\\trainr2021_mask.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 1 feature and 1 field\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 518189.1 ymin: 2776305 xmax: 521241.5 ymax: 2780157\n## Projected CRS: NAD83 / UTM zone 17N\n#create list to put results into\nrand_list &lt;- list()\n\n\nfor (i in 1:length(COA_list)) {\n  # For reproducibility.\n  set.seed(19) \n  \n  # Distribute x number of points across defined available resource unit for this particular transmitter, year, diel period combination.\n  randLocs &lt;- sf::st_sample(extent, size = COA_list[[i]]$count, type = 'random') %&gt;% st_transform(2958) %&gt;% sfc_to_df()\n  \n  set.seed(19)\n  #  Get and randomize coordinates.\n  xcoor &lt;- as.data.frame(randLocs[,3])\n  ycoor &lt;- as.data.frame(randLocs[,4])\n  # Randomize coordinates.\n  x.random &lt;- as.data.frame(xcoor[sample(1:nrow(xcoor)), ])\n  y.random &lt;- as.data.frame(ycoor[sample(1:nrow(ycoor)), ])\n  coords.random &lt;- as.data.frame(c(x.random, y.random))\n  names(coords.random) &lt;- c('x', 'y')\n  # Make a data frame that matches the number of COAs for that individual.\n  df &lt;- do.call('rbind', COA_list[i]) \n  # Replicate the info to match the observed.\n  df2 &lt;- rbind(df, df[rep(1, (df$count - 1)), ]) \n  # Delete row names.\n  rownames(df2) &lt;- NULL \n  df2$x &lt;- coords.random[, 1]\n  # Put the coordinates from the random sample into the data frame.\n  df2$y &lt;- coords.random[, 2] \n  #convert df2 into dataframe\n  #df2 &lt;- df2 %&gt;% st_to_sf()\n  # Label these detections are background points (0) (opposed to observed COAs (1)).\n  df2$RealDets &lt;- 0\n  \n  \n  \n  # Place completed iteration into a list.\n  rand_list[[i]] &lt;- df2\n}\n\nRandomPts &lt;- as.data.frame(do.call(\"rbind\", rand_list)) #make the list into a dataframe\n\n#combine real COAs with pseudo-absences\ncoadat1$RealDets &lt;- 1\ncoadat1 &lt;- coadat1 %&gt;% mutate(TYD = paste0(Transmitter, \"_\", period, \"_\", periody))\n#remove data that we do not have random points for (combinations with less than 5 COAs)\ncoadat1 &lt;- coadat1 %&gt;% filter(TYD %in% RandomPts$TYD)\n\n#make dataframes have same columns\ncoadat1 &lt;- coadat1 %&gt;% dplyr::select(-c(DateTime, Date, Time, Year, Month, Day, Hour, Minute, Second))\nRandomPts &lt;- RandomPts %&gt;% dplyr::select(-count)\n\nalldat &lt;- rbind(coadat1, RandomPts)\nhead(alldat)\n##       Transmitter        x       y period periody RealDets\n##            &lt;char&gt;    &lt;num&gt;   &lt;num&gt; &lt;char&gt;  &lt;char&gt;    &lt;num&gt;\n## 1: A69-1602-37787 518196.7 2779938    Day       w        1\n## 2: A69-1602-37787 518203.1 2780157   Dusk      ew        1\n## 3: A69-1602-37787 518203.1 2780157   Dusk      ew        1\n## 4: A69-1602-37783 518203.1 2780157   Dawn      ew        1\n## 5: A69-1602-37798 518203.1 2780157    Day       w        1\n## 6: A69-1602-37783 518203.1 2780157    Day      ew        1\n##                       TYD\n##                    &lt;char&gt;\n## 1:   A69-1602-37787_Day_w\n## 2: A69-1602-37787_Dusk_ew\n## 3: A69-1602-37787_Dusk_ew\n## 4: A69-1602-37783_Dawn_ew\n## 5:   A69-1602-37798_Day_w\n## 6:  A69-1602-37783_Day_ew"
  },
  {
    "objectID": "RF.html#adding-environmental-data",
    "href": "RF.html#adding-environmental-data",
    "title": "RSF with Random Forest Models",
    "section": "Adding environmental data",
    "text": "Adding environmental data\nNow we have our presence/absence dataset! Now we need to add environmental variables as predictors of the presence/absence. These can be anything from depth contours to benthic habitat data to water quality parameters. But be careful to think about scale.\nThe environmental data we are using for this exercise is benthic habitat data from field surveys and includes:\n - tt: Total Thalassia Cover\n - hw: Total Halodule Cover\n - cov: Total SAV Cover\n - sdcov: Standard Deviation of SAV Cover\n - numsp: Number of Species\nThese variables are interpolated across the array using Kriging at a spacial resolution of 75m, since that is the average detection range of the receivers. Data is in a raster format\nWe then can make the presence/absence data into a spatial dataframe and extract the values from the rasters.\n\n#now we have our full dataset with presences and pseudo-absences!!! \n#Now we can extract environmental variables to model habitat selection\n#Load in rasters - all rasters are interpolated maps from either surveys performed by Rodemann et al. or by FWRI as part of the Fisheries Habitat Assessment Program (FHAP) in Florida Bay\ncov_2020 &lt;- rast('data/cov2020.tif') #percent SAV cover\nsdcov_2020 &lt;- rast('data/sdcov2020.tif') #standard deviation of cover\nnumsp_2020 &lt;- rast('data/num2020.tif') #number of SAV species\nhw_2020 &lt;- rast('data/hw2020.tif') #Halodule wrightii cover\ntt_2020 &lt;- rast('data/tt2020.tif') #Thalassia testudinum cover\n\n#crop all rasters to same extent\n#load in shapefile for extent\nextent &lt;- st_read('data/trainr2021_mask.shp')\n## Reading layer `trainr2021_mask' from data source \n##   `C:\\Users\\jonro\\OneDrive\\Desktop\\RSF_OTN_Workshop\\RSF_OTN_Workshop\\data\\trainr2021_mask.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 1 feature and 1 field\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 518189.1 ymin: 2776305 xmax: 521241.5 ymax: 2780157\n## Projected CRS: NAD83 / UTM zone 17N\n\ncov2020 &lt;- terra::crop(cov_2020, extent)\nsdcov2020 &lt;- terra::crop(sdcov_2020, extent)\nnum2020 &lt;- terra::crop(numsp_2020, extent)\nhw2020 &lt;- terra::crop(hw_2020, extent)\ntt2020 &lt;- terra::crop(tt_2020, extent)\n\n\n#We have all the rasters at the same spatial extent. Now stack them to get ready for extraction\nrastdat &lt;- c(cov2020, sdcov2020, num2020, hw2020, tt2020)\nrastdat &lt;- terra::project(rastdat, 'epsg:2958')\n\n#extract habitat data at each point - need to put the points into a spatial format first\ndatcoor &lt;- alldat %&gt;% \n  st_as_sf(coords = c('x', 'y')) %&gt;% #set up the coordinates\n  st_set_crs(2958) # using 2958 for projected\n\ndatextract &lt;- terra::extract(rastdat, datcoor) #extract data at each point\ndatrf &lt;- cbind(datextract, alldat) %&gt;% drop_na() #combine dataframes and remove NAs (only happens if cell is not kept within cropped raster)\nhead(datrf)\n##   ID  cov2020 sdcov2020  num2020   hw2020      tt2020    Transmitter        x\n## 1  1 43.87477  22.71996 1.910579 17.81729  0.06158432 A69-1602-37787 518196.7\n## 2 12 46.15767  18.92309 1.838648 13.38963  2.98668218 A69-1602-37787 518789.0\n## 3 22 57.52768  24.01185 2.435778 16.14622  0.22619982 A69-1602-37779 518190.9\n## 4 23 57.52768  24.01185 2.435778 16.14622  0.22619982 A69-1602-37779 518190.4\n## 5 24 57.52768  24.01185 2.435778 16.14622  0.22619982 A69-1602-37790 518190.4\n## 6 25 52.58038  25.14360 2.273478 19.32028 -0.64172739 A69-1602-37771 518195.6\n##         y period periody RealDets                    TYD\n## 1 2779938    Day       w        1   A69-1602-37787_Day_w\n## 2 2780079   Dawn       w        1  A69-1602-37787_Dawn_w\n## 3 2778707   Dawn      ed        1 A69-1602-37779_Dawn_ed\n## 4 2778691    Day      ed        1  A69-1602-37779_Day_ed\n## 5 2778691    Day       w        1   A69-1602-37790_Day_w\n## 6 2778855    Day      ed        1  A69-1602-37771_Day_ed"
  },
  {
    "objectID": "RF.html#modelling-with-random-forest",
    "href": "RF.html#modelling-with-random-forest",
    "title": "RSF with Random Forest Models",
    "section": "Modelling with Random Forest",
    "text": "Modelling with Random Forest\nNow we can finally get into modelling with Random Forests! We will be using the package mlr3, a very flexible package that creates wrappers for other machine learning packages. Then mlr3 aids with training the model and interpretation.\nFirst step is to select the training/testing data. This allows you to use a different dataset to evaluate your model. We chose a 70/30 split. We then turn the datasets into a spatial dataset to run RF with spatial considerations.\n\n#datrf is the dataset that we will use for all of our models. We have temporal components we can put into GLMM and GAMs as well as individual data for random effects\n#let's get into modelling this with rf!\n\n#need to remove all columns that we are not using for now\ndatrf &lt;- datrf %&gt;% dplyr::select(-c(Transmitter, period, periody, TYD))\n\n#Set seed for replications\nset.seed(19)\n\ndatrf$RealDets &lt;- as.factor(datrf$RealDets)\n\n# Randomly select 70% of the data frame for the training dataset\nRSF_ar.train &lt;- datrf[sample(1:nrow(datrf), nrow(datrf) * 0.7, replace = FALSE), ]\n# Remainder (i.e., 30%) becomes the test dataset.\nRSF_ar.test &lt;- datrf[!(datrf$ID %in% RSF_ar.train$ID), ] \n\n\n#take out ID column\nRSF_ar.test &lt;- RSF_ar.test %&gt;% dplyr::select(-ID) %&gt;% drop_na()\nRSF_ar.train &lt;- RSF_ar.train %&gt;% dplyr::select(-ID) %&gt;% drop_na()\n\n\n#turn datasets into sf objects for spatial classification\nRSF_ar.train1 &lt;- RSF_ar.train %&gt;% as_tibble() %&gt;% \n  st_as_sf(coords = c('x', 'y')) %&gt;% #set up the coordinates\n  st_set_crs(2958) \n\nRSF_ar.test1 &lt;- RSF_ar.test %&gt;% \n  st_as_sf(coords = c('x', 'y')) %&gt;% #set up the coordinates\n  st_set_crs(2958)"
  },
  {
    "objectID": "RF.html#training-tuning-and-running-rf-model",
    "href": "RF.html#training-tuning-and-running-rf-model",
    "title": "RSF with Random Forest Models",
    "section": "Training, tuning, and running RF model",
    "text": "Training, tuning, and running RF model\nWe are all set up! Are you excited? Well, first we need to set up learners for running the Randon Forest model and then tune what are called “hyperparameters.” Hyperparameters are parameters (imagine that) that set up how the RF model runs. These include things like mtry (# of predictor variables in each tree), sample fraction (fraction of observations to be used in each tree), and minimum node size (minimum number of datapoints in a node). Tuning hyperparameters can take a while because it is a random or grid search that minimized the out-of-box error. This example should only take 5 minutes to run, but you can get fancy and have the tuning run for multiple hours to get the best model that you can.\n\n# Set tasks for training and test datasets.\ntask_trout.train &lt;- as_task_classif_st(\n  RSF_ar.train1, target = \"RealDets\", positive = '1',\n)\nstr(task_trout.train)\n## Classes 'TaskClassifST', 'TaskClassif', 'TaskSupervised', 'Task', 'R6' &lt;TaskClassifST:RSF_ar.train1&gt;\ntask_trout.train\n## &lt;TaskClassifST:RSF_ar.train1&gt; (2763 x 6)\n## * Target: RealDets\n## * Properties: twoclass\n## * Features (5):\n##   - dbl (5): cov2020, hw2020, num2020, sdcov2020, tt2020\n## * Coordinates:\n##              X       Y\n##          &lt;num&gt;   &lt;num&gt;\n##    1: 521005.5 2779492\n##    2: 520997.9 2778977\n##    3: 518816.5 2779197\n##    4: 518920.6 2778294\n##    5: 518387.8 2777374\n##   ---                 \n## 2759: 519425.4 2778406\n## 2760: 520628.2 2777985\n## 2761: 518397.9 2779850\n## 2762: 518189.1 2779674\n## 2763: 519432.4 2778212\n\ntask_trout.test &lt;- as_task_classif_st(\n  x = RSF_ar.test1, target = \"RealDets\",\n  positive = \"1\"\n)\n\n# Make learner.\nlearner &lt;-lrn(\n  \"classif.ranger\",\n  predict_type = \"prob\",\n  mtry  = to_tune(1, ncol(RSF_ar.train) - 4),\n  sample.fraction = to_tune(0.2, 0.9),\n  min.node.size = to_tune(1,10),\n  importance = 'impurity'\n)\n\n#tune hyperparameters\ninstance = ti(\n  task = task_trout.train,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\n\ntuner = mlr3tuning::tnr(\"grid_search\", resolution = 2, batch_size = 2)\ntuner$optimize(instance) # Takes ~ 4 minutes on my relatively fast computer\n## INFO  [09:43:34.132] [bbotk] Starting to optimize 3 parameter(s) with '&lt;OptimizerBatchGridSearch&gt;' and '&lt;TerminatorNone&gt;'\n## INFO  [09:43:34.179] [bbotk] Evaluating 2 configuration(s)\n## INFO  [09:43:34.197] [mlr3] Running benchmark with 10 resampling iterations\n## INFO  [09:43:34.230] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 1/5)\n## INFO  [09:43:35.095] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 2/5)\n## INFO  [09:43:35.895] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 3/5)\n## INFO  [09:43:36.681] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 4/5)\n## INFO  [09:43:37.492] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 5/5)\n## INFO  [09:43:38.283] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 1/5)\n## INFO  [09:43:38.586] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 2/5)\n## INFO  [09:43:39.213] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 3/5)\n## INFO  [09:43:39.535] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 4/5)\n## INFO  [09:43:39.860] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 5/5)\n## INFO  [09:43:40.184] [mlr3] Finished benchmark\n## INFO  [09:43:40.234] [bbotk] Result of batch 1:\n## INFO  [09:43:40.237] [bbotk]  min.node.size mtry sample.fraction classif.ce warnings errors runtime_learners\n## INFO  [09:43:40.237] [bbotk]              1    4             0.9 0.09265862        0      0             4.01\n## INFO  [09:43:40.237] [bbotk]             10    1             0.2 0.10207432        0      0             1.55\n## INFO  [09:43:40.237] [bbotk]                                 uhash\n## INFO  [09:43:40.237] [bbotk]  22f9b660-351e-454c-a266-8e61173f78a9\n## INFO  [09:43:40.237] [bbotk]  b76571b2-09ab-4660-91cf-a7c6d4e8f60f\n## INFO  [09:43:40.243] [bbotk] Evaluating 2 configuration(s)\n## INFO  [09:43:40.252] [mlr3] Running benchmark with 10 resampling iterations\n## INFO  [09:43:40.258] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 1/5)\n## INFO  [09:43:40.743] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 2/5)\n## INFO  [09:43:41.229] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 3/5)\n## INFO  [09:43:41.728] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 4/5)\n## INFO  [09:43:42.203] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 5/5)\n## INFO  [09:43:42.664] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 1/5)\n## INFO  [09:43:43.404] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 2/5)\n## INFO  [09:43:44.144] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 3/5)\n## INFO  [09:43:44.876] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 4/5)\n## INFO  [09:43:45.602] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 5/5)\n## INFO  [09:43:46.309] [mlr3] Finished benchmark\n## INFO  [09:43:46.367] [bbotk] Result of batch 2:\n## INFO  [09:43:46.370] [bbotk]  min.node.size mtry sample.fraction classif.ce warnings errors runtime_learners\n## INFO  [09:43:46.370] [bbotk]             10    1             0.9 0.09592342        0      0             2.38\n## INFO  [09:43:46.370] [bbotk]             10    4             0.9 0.09411314        0      0             3.59\n## INFO  [09:43:46.370] [bbotk]                                 uhash\n## INFO  [09:43:46.370] [bbotk]  afe15533-09e0-4f62-8d0b-747d7414b48a\n## INFO  [09:43:46.370] [bbotk]  d1ac5fdd-44c5-4187-b57c-3328bacd3d36\n## INFO  [09:43:46.375] [bbotk] Evaluating 2 configuration(s)\n## INFO  [09:43:46.393] [mlr3] Running benchmark with 10 resampling iterations\n## INFO  [09:43:46.398] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 1/5)\n## INFO  [09:43:46.753] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 2/5)\n## INFO  [09:43:47.114] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 3/5)\n## INFO  [09:43:47.466] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 4/5)\n## INFO  [09:43:47.826] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 5/5)\n## INFO  [09:43:48.194] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 1/5)\n## INFO  [09:43:48.598] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 2/5)\n## INFO  [09:43:49.007] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 3/5)\n## INFO  [09:43:49.407] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 4/5)\n## INFO  [09:43:49.799] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 5/5)\n## INFO  [09:43:50.237] [mlr3] Finished benchmark\n## INFO  [09:43:50.294] [bbotk] Result of batch 3:\n## INFO  [09:43:50.297] [bbotk]  min.node.size mtry sample.fraction classif.ce warnings errors runtime_learners\n## INFO  [09:43:50.297] [bbotk]              1    1             0.2 0.09773371        0      0             1.74\n## INFO  [09:43:50.297] [bbotk]             10    4             0.2 0.10569751        0      0             1.99\n## INFO  [09:43:50.297] [bbotk]                                 uhash\n## INFO  [09:43:50.297] [bbotk]  20c1a644-1d07-4316-a917-8ecf39712420\n## INFO  [09:43:50.297] [bbotk]  7227eba1-4a68-49c9-80d5-69dc5e08e7dd\n## INFO  [09:43:50.300] [bbotk] Evaluating 2 configuration(s)\n## INFO  [09:43:50.311] [mlr3] Running benchmark with 10 resampling iterations\n## INFO  [09:43:50.319] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 1/5)\n## INFO  [09:43:50.938] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 2/5)\n## INFO  [09:43:51.610] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 3/5)\n## INFO  [09:43:52.177] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 4/5)\n## INFO  [09:43:52.729] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 5/5)\n## INFO  [09:43:53.247] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 1/5)\n## INFO  [09:43:53.695] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 2/5)\n## INFO  [09:43:54.131] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 3/5)\n## INFO  [09:43:54.565] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 4/5)\n## INFO  [09:43:54.991] [mlr3] Applying learner 'classif.ranger' on task 'RSF_ar.train1' (iter 5/5)\n## INFO  [09:43:55.449] [mlr3] Finished benchmark\n## INFO  [09:43:55.501] [bbotk] Result of batch 4:\n## INFO  [09:43:55.504] [bbotk]  min.node.size mtry sample.fraction classif.ce warnings errors runtime_learners\n## INFO  [09:43:55.504] [bbotk]              1    1             0.9 0.09193660        0      0             2.86\n## INFO  [09:43:55.504] [bbotk]              1    4             0.2 0.09845965        0      0             2.15\n## INFO  [09:43:55.504] [bbotk]                                 uhash\n## INFO  [09:43:55.504] [bbotk]  e0038ab3-4dc5-4c7f-9d2a-301c99cb3837\n## INFO  [09:43:55.504] [bbotk]  141b77dd-4ca1-430a-aaa7-a624cb46f592\n## INFO  [09:43:55.525] [bbotk] Finished optimizing after 8 evaluation(s)\n## INFO  [09:43:55.526] [bbotk] Result:\n## INFO  [09:43:55.528] [bbotk]  min.node.size  mtry sample.fraction learner_param_vals  x_domain classif.ce\n## INFO  [09:43:55.528] [bbotk]          &lt;int&gt; &lt;int&gt;           &lt;num&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;\n## INFO  [09:43:55.528] [bbotk]              1     1             0.9          &lt;list[5]&gt; &lt;list[3]&gt;  0.0919366\n##    min.node.size  mtry sample.fraction learner_param_vals  x_domain classif.ce\n##            &lt;int&gt; &lt;int&gt;           &lt;num&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;\n## 1:             1     1             0.9          &lt;list[5]&gt; &lt;list[3]&gt;  0.0919366\nbeep(1)\n\n#store tuned hyperparameters in learner\nlearner$param_set$values &lt;- instance$result_learner_param_vals\n\nNow we can train and view the model!\n\n#finally! We can train our model with the train function\nlearner$train(task_trout.train)\n\n#let's quickly look at the model\nlearner$model\n## Ranger result\n## \n## Call:\n##  ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", case.weights = task$weights$weight,      importance = \"impurity_corrected\", min.node.size = 1L, mtry = 4L,      num.threads = 1L, sample.fraction = 0.9) \n## \n## Type:                             Probability estimation \n## Number of trees:                  500 \n## Sample size:                      2763 \n## Number of independent variables:  5 \n## Mtry:                             4 \n## Target node size:                 1 \n## Variable importance mode:         impurity_corrected \n## Splitrule:                        gini \n## OOB prediction error (Brier s.):  0.08508471\n\nAnd finally, let’s look at the accuracy of the model\n\nmeasures &lt;- msrs(c('classif.acc'))\n\npred_train &lt;- learner$predict(task_trout.train)\n## Warning in predict.ranger(self$model, data = newdata, predict.type =\n## \"response\", : Forest was grown with 'impurity_corrected' variable importance.\n## For prediction it is advised to grow another forest without this importance\n## setting.\npred_train$confusion\n##         truth\n## response    1    0\n##        1 1149   37\n##        0  215 1362\npred_train$score(measures)\n## classif.acc \n##   0.9087948\n\n\npred_test &lt;- learner$predict(task_trout.test)\n## Warning in predict.ranger(self$model, data = newdata, predict.type =\n## \"response\", : Forest was grown with 'impurity_corrected' variable importance.\n## For prediction it is advised to grow another forest without this importance\n## setting.\npred_test$confusion\n##         truth\n## response   1   0\n##        1 476  24\n##        0  82 603\npred_test$score(measures)\n## classif.acc \n##   0.9105485"
  },
  {
    "objectID": "RF.html#interpretation-of-the-model",
    "href": "RF.html#interpretation-of-the-model",
    "title": "RSF with Random Forest Models",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\nNow comes the truly fun part: interpreting your results! This is done in three steps. The first step is to look at variable importance. This ranks all of the predictor variables in order of most to least influencial on the model results. The top predictor variable is thus the “most important” variable for predicting habitat selection.\nThe second step is looking at the marginal effects plots. These plots illustrate how one predictor variable impacts the probability of selection when all other predictor variables are held constant. We will go through 3 variables to discuss\nAnd finally the third way of visualizing the results is of course across space. You can do this by layering the rasters of your predictor variables and using your model to predict probability of presence. The outcome from these data is going to be a little weird, but that is probably because of the sample size and the variables we are using. Usually you want more data for this sort of analysis!\nThis code is not working for the website, so we will go to the actual code for this\nCongratulations, you’ve now worked through an example of going from detections to implementing RSFs through Random Forest. Now we will implement the same approach with a Generalized Linear Mixed-Effect Model (GLMM) using the glmmTMB package in the next example. GLMMs offer easier interpretability, handle random effects (i.e., individual variation), and can better account for autocorrelation compared to Random Forest."
  },
  {
    "objectID": "GLMM.html",
    "href": "GLMM.html",
    "title": "RSF with GLMM",
    "section": "",
    "text": "This vignette usesGLMMs to calculate Resource Selection Functions. This section uses the same setup as the first tab, so we will just start with brining in that data\n\nlibrary(glmmTMB)   # for fitting GLMMs\nlibrary(cowplot)   # for plotting multiple panels together via plot_grid function\nlibrary(ggeffects) # for extracting the marginal effects of each covariate in the model\nlibrary(MuMIn)   # for AIC\nlibrary(performance) # for R2\nlibrary(terra) #raster\nlibrary(tidyverse)\nlibrary(raster)\nlibrary(sf)\nlibrary(data.table)\n## \n## Attaching package: 'data.table'\n## The following object is masked from 'package:raster':\n## \n##     shift\n## The following objects are masked from 'package:lubridate':\n## \n##     hour, isoweek, mday, minute, month, quarter, second, wday, week,\n##     yday, year\n## The following objects are masked from 'package:dplyr':\n## \n##     between, first, last\n## The following object is masked from 'package:purrr':\n## \n##     transpose\n## The following object is masked from 'package:terra':\n## \n##     shift\n\n# Ensure the dataset has the proper format with predictors and the response variable (RealDets)\ndatextract &lt;- readRDS('data/datextract.RDS')\nalldat &lt;- readRDS('data/alldat.RDS')\n# As before, assemble our dataset remove NAs\ndatglmm &lt;- cbind(datextract, alldat) %&gt;% drop_na() %&gt;% mutate(RealDets = as.factor(RealDets), Transmitter = as.factor(Transmitter))"
  },
  {
    "objectID": "GLMM.html#rsfs-with-glmm",
    "href": "GLMM.html#rsfs-with-glmm",
    "title": "RSF with GLMM",
    "section": "",
    "text": "This vignette usesGLMMs to calculate Resource Selection Functions. This section uses the same setup as the first tab, so we will just start with brining in that data\n\nlibrary(glmmTMB)   # for fitting GLMMs\nlibrary(cowplot)   # for plotting multiple panels together via plot_grid function\nlibrary(ggeffects) # for extracting the marginal effects of each covariate in the model\nlibrary(MuMIn)   # for AIC\nlibrary(performance) # for R2\nlibrary(terra) #raster\nlibrary(tidyverse)\nlibrary(raster)\nlibrary(sf)\nlibrary(data.table)\n## \n## Attaching package: 'data.table'\n## The following object is masked from 'package:raster':\n## \n##     shift\n## The following objects are masked from 'package:lubridate':\n## \n##     hour, isoweek, mday, minute, month, quarter, second, wday, week,\n##     yday, year\n## The following objects are masked from 'package:dplyr':\n## \n##     between, first, last\n## The following object is masked from 'package:purrr':\n## \n##     transpose\n## The following object is masked from 'package:terra':\n## \n##     shift\n\n# Ensure the dataset has the proper format with predictors and the response variable (RealDets)\ndatextract &lt;- readRDS('data/datextract.RDS')\nalldat &lt;- readRDS('data/alldat.RDS')\n# As before, assemble our dataset remove NAs\ndatglmm &lt;- cbind(datextract, alldat) %&gt;% drop_na() %&gt;% mutate(RealDets = as.factor(RealDets), Transmitter = as.factor(Transmitter))"
  },
  {
    "objectID": "GLMM.html#prepare-data",
    "href": "GLMM.html#prepare-data",
    "title": "RSF with GLMM",
    "section": "Prepare data",
    "text": "Prepare data\nSince we just put together all the data, we will prepare the training and testing data. Again, a 70/30 split\n\n# Derive the training and testing dataset\nset.seed(19)\n\n# Add a unique ID column to each row\ndatglmm &lt;- datglmm %&gt;%\n  mutate(ID = seq_len(nrow(datglmm)))\n\n# Split the data: 70% training, 30% testing\nRSF_ar.train &lt;- datglmm %&gt;%\n  group_by(Transmitter) %&gt;%\n  sample_frac(0.7) %&gt;% \n  ungroup() %&gt;% \n  as.data.frame()\n\nRSF_ar.test &lt;- anti_join(datglmm, RSF_ar.train, by = \"ID\")\n\ntable(RSF_ar.train$Transmitter)\n## \n## A69-1602-37380 A69-1602-37771 A69-1602-37773 A69-1602-37774 A69-1602-37778 \n##            636             84            246            214            310 \n## A69-1602-37779 A69-1602-37783 A69-1602-37787 A69-1602-37788 A69-1602-37790 \n##            280            117            202             78            471 \n## A69-1602-37798 \n##            127\ntable(RSF_ar.test$Transmitter)\n## \n## A69-1602-37380 A69-1602-37771 A69-1602-37773 A69-1602-37774 A69-1602-37778 \n##            272             36            105             91            133 \n## A69-1602-37779 A69-1602-37783 A69-1602-37787 A69-1602-37788 A69-1602-37790 \n##            120             50             86             33            202 \n## A69-1602-37798 \n##             55"
  },
  {
    "objectID": "GLMM.html#fitting-and-summarizing-the-model",
    "href": "GLMM.html#fitting-and-summarizing-the-model",
    "title": "RSF with GLMM",
    "section": "Fitting and summarizing the model",
    "text": "Fitting and summarizing the model\nGLMMs are a lot more flexible in terms of the model terms that can be used. Therefore, we can put random effects such as individuals into the model. Furthermore, we can use polynomials if needed.\nWe choose to go for the polynomial model in the following code because of a lower AICc score\n\n# Fit a GLMM with glmmTMB\nglmmTMB_model_lin &lt;- glmmTMB(RealDets ~  hw2020 + tt2020 + cov2020 + num2020 + sdcov2020 + (1 | Transmitter),\n                         data = RSF_ar.train, family = binomial)\n\nglmmTMB_model_poly &lt;- glmmTMB(RealDets ~  hw2020 + tt2020 + poly(cov2020,2) + num2020 + sdcov2020 +(1 | Transmitter),\n                         data = RSF_ar.train, family = binomial)\n\nAICc(glmmTMB_model_lin, glmmTMB_model_poly)\n##                    df     AICc\n## glmmTMB_model_lin   7 3359.324\n## glmmTMB_model_poly  8 3177.352\n\nglmmTMB_model = glmmTMB_model_poly\n# Summarize the model results\nsummary(glmmTMB_model)\n##  Family: binomial  ( logit )\n## Formula:          \n## RealDets ~ hw2020 + tt2020 + poly(cov2020, 2) + num2020 + sdcov2020 +  \n##     (1 | Transmitter)\n## Data: RSF_ar.train\n## \n##      AIC      BIC   logLik deviance df.resid \n##   3177.3   3224.7  -1580.6   3161.3     2757 \n## \n## Random effects:\n## \n## Conditional model:\n##  Groups      Name        Variance Std.Dev.\n##  Transmitter (Intercept) 0.06858  0.2619  \n## Number of obs: 2765, groups:  Transmitter, 11\n## \n## Conditional model:\n##                    Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)        -6.13545    0.64936  -9.448  &lt; 2e-16 ***\n## hw2020             -0.06355    0.02304  -2.758  0.00581 ** \n## tt2020             -0.01097    0.01389  -0.790  0.42948    \n## poly(cov2020, 2)1 -40.60981    3.32620 -12.209  &lt; 2e-16 ***\n## poly(cov2020, 2)2  34.02053    2.60794  13.045  &lt; 2e-16 ***\n## num2020            -0.03780    0.15760  -0.240  0.81044    \n## sdcov2020           0.36540    0.02050  17.828  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nr2(glmmTMB_model) \n## # R2 for Mixed Models\n## \n##   Conditional R2: 0.323\n##      Marginal R2: 0.309\n# Marginal R2 focuses on the explanatory power of the fixed effects alone.\n# Conditional R2 considers both fixed effects and random effects. Only slightly improves variance explained."
  },
  {
    "objectID": "GLMM.html#model-accuracy",
    "href": "GLMM.html#model-accuracy",
    "title": "RSF with GLMM",
    "section": "Model accuracy",
    "text": "Model accuracy\nLet’s look at the model accuracy. You will see it is a lot lower than the RF model\n\n\nRSF_ar.train &lt;- readRDS('data/RSF_train.rds')\nRSF_ar.test &lt;- readRDS('data/RSF_test.rds')\n\n# Accuracy of the model on the training data\nRSF_ar.train$predicted_probs_glmm &lt;- predict(glmmTMB_model, newdata = RSF_ar.train, type = \"response\")\nRSF_ar.train$predicted_class_glmm &lt;- ifelse(RSF_ar.train$predicted_probs_glmm &gt; 0.5, 1, 0)\ntrain_conf_matrix &lt;- table(RSF_ar.train$RealDets, RSF_ar.train$predicted_class_glmm)\ntrain_conf_matrix\n##    \n##       0   1\n##   0 889 510\n##   1 493 871\n\n# Accuracy of the model on the test data\nRSF_ar.test$predicted_probs_glmm &lt;- predict(glmmTMB_model, newdata = RSF_ar.test, type = \"response\")\nRSF_ar.test$predicted_class_glmm &lt;- ifelse(RSF_ar.test$predicted_probs_glmm &gt; 0.5, 1, 0)\ntest_conf_matrix &lt;- table(RSF_ar.test$RealDets, RSF_ar.test$predicted_class_glmm)\ntest_conf_matrix\n##    \n##       0   1\n##   0 397 230\n##   1 199 359\n\n# Calculate accuracy metrics for both training and test datasets\n(train_accuracy &lt;- sum(diag(train_conf_matrix)) / sum(train_conf_matrix))\n## [1] 0.6369888\n(test_accuracy &lt;- sum(diag(test_conf_matrix)) / sum(test_conf_matrix))\n## [1] 0.6379747"
  },
  {
    "objectID": "GLMM.html#model-interpretation",
    "href": "GLMM.html#model-interpretation",
    "title": "RSF with GLMM",
    "section": "Model Interpretation",
    "text": "Model Interpretation\nLet’s get into model interpretation! Similar steps to RF\n\n# Calculate marginal effects for hw, tt\neffects_hw2020 &lt;- ggpredict(glmmTMB_model, terms = \"hw2020 [all]\")\neffects_tt2020 &lt;- ggpredict(glmmTMB_model, terms = \"tt2020 [all]\")\neffects_sdcov2020 &lt;- ggpredict(glmmTMB_model, terms = \"sdcov2020 [all]\")\neffects_num2020 &lt;- ggpredict(glmmTMB_model, terms = \"num2020 [all]\")\n\n\n# Plot marginal effects for each covariate\nhh_marg &lt;- ggplot(effects_hw2020, aes(x = x, y = predicted)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(x = \"Halodule wrightii Cover\", y = \"Predicted Probability\") +\n  theme_minimal()\n\ntt_marg &lt;- ggplot(effects_tt2020, aes(x = x, y = predicted)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(x = \"Thalassia testudinum Cover\", y = \"Predicted Probability\") +\n  theme_minimal()\n\nsdcov_marg &lt;- ggplot(effects_sdcov2020, aes(x = x, y = predicted)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(x = \"Standard Deviation in SAV Cover\", y = \"Predicted Probability\") +\n  theme_minimal()\n\nnum_marg &lt;- ggplot(effects_num2020, aes(x = x, y = predicted)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(x = \"Number of Species\", y = \"Predicted Probability\") +\n  theme_minimal()\n\n\n# Combine the 4 plots into one using cowplot\ncowplot::plot_grid(hh_marg, tt_marg, sdcov_marg, num_marg, labels = \"auto\", ncol = 4)\n\n\n\n\n\n\n\n\n\n#now onto prediction mapping!\n\ncov_2020 &lt;- rast('data/cov2020.tif') #percent SAV cover\nsdcov_2020 &lt;- rast('data/sdcov2020.tif') #standard deviation of cover\nnumsp_2020 &lt;- rast('data/num2020.tif') #number of SAV species\nhw_2020 &lt;- rast('data/hw2020.tif') #Halodule wrightii cover\ntt_2020 &lt;- rast('data/tt2020.tif')\n\nextent &lt;- st_read('data/trainr2021_mask.shp')\n## Reading layer `trainr2021_mask' from data source \n##   `C:\\Users\\jonro\\OneDrive\\Desktop\\RSF_OTN_Workshop\\RSF_OTN_Workshop\\data\\trainr2021_mask.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 1 feature and 1 field\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 518189.1 ymin: 2776305 xmax: 521241.5 ymax: 2780157\n## Projected CRS: NAD83 / UTM zone 17N\n\ncov2020 &lt;- terra::crop(cov_2020, extent)\nsdcov2020 &lt;- terra::crop(sdcov_2020, extent)\nnum2020 &lt;- terra::crop(numsp_2020, extent)\nhw2020 &lt;- terra::crop(hw_2020, extent)\ntt2020 &lt;- terra::crop(tt_2020, extent)\n\nrastdat &lt;- c(cov2020, sdcov2020, num2020, hw2020, tt2020)\nrastdat &lt;- terra::project(rastdat, 'epsg:2958')\n\n# Predict onto the spatial grid for GLMM (similar to Random Forest)\nnewdata &lt;- as.data.table(as.data.frame(rastdat)) %&gt;% \n  mutate(Transmitter = \"place-holder\") # have to include the column of Transmitter, even though we will ignore it in predictions.\n\n# Predict probabilities of presence for the grid, excluding random effects\nnewdata$predicted_probs_glmm &lt;- predict(glmmTMB_model, newdata = newdata, type = \"response\", \n                                        re.form = NA) # Ignore random effects.\n## Warning in checkTerms(data.tmb1$terms, data.tmb0$terms): Predicting new random effect levels for terms: 1 | Transmitter\n## Disable this warning with 'allow.new.levels=TRUE'\n\n# Map the predicted probabilities\nnew_raster &lt;- rast(ext(rastdat), resolution = res(rastdat), crs = crs(rastdat))\nnew_raster[] &lt;- newdata$predicted_probs_glmm\n\n# Plot the predicted probabilities\nplot(new_raster, main = \"GLMM Predicted Probabilities of Presence\")\n\n\n\n\n\n\n\n\n\n# Convert rasters to data frames\ndf_glmm &lt;- as.data.frame(new_raster, xy = TRUE)\ncolnames(df_glmm)[3] &lt;- \"GLMM_Prob\"\n\n# Create the GLMM plot\n(glmm_plot &lt;- ggplot(df_glmm, aes(x = x, y = y, fill = GLMM_Prob)) +\n  geom_tile() +\n  scale_fill_viridis_c(limits = c(0, 1), name = \"Probability\") + \n  coord_equal() +\n  labs(title = \"GLMM Predicted Probabilities of Presence\") +\n  theme_minimal())\n\n\n\n\n\n\n\n\nCongrats, you have used GLMMs now as well!"
  },
  {
    "objectID": "GAM.html",
    "href": "GAM.html",
    "title": "RSF with GAMs",
    "section": "",
    "text": "This vignette uses GAMs to calculate Resource Selection Functions. This section uses the same setup as the first tab, so we will just start with brining in that data\n\nlibrary(glmmTMB)   # for fitting GLMMs\nlibrary(cowplot)   # for plotting multiple panels together via plot_grid function\nlibrary(ggeffects) # for extracting the marginal effects of each covariate in the model\nlibrary(MuMIn)   # for AIC\nlibrary(performance) # for R2\nlibrary(terra) #raster\nlibrary(tidyverse)\nlibrary(raster)\nlibrary(sf)\nlibrary(data.table)\nlibrary(mgcv)\n\n# Ensure the dataset has the proper format with predictors and the response variable (RealDets)\ndatextract &lt;- readRDS('data/datextract.RDS')\nalldat &lt;- readRDS('data/alldat.RDS')\n# As before, assemble our dataset remove NAs\ndatgam &lt;- cbind(datextract, alldat) %&gt;% drop_na() %&gt;% mutate(RealDets = as.factor(RealDets), Transmitter = as.factor(Transmitter))"
  },
  {
    "objectID": "GAM.html#rsfs-with-gams",
    "href": "GAM.html#rsfs-with-gams",
    "title": "RSF with GAMs",
    "section": "",
    "text": "This vignette uses GAMs to calculate Resource Selection Functions. This section uses the same setup as the first tab, so we will just start with brining in that data\n\nlibrary(glmmTMB)   # for fitting GLMMs\nlibrary(cowplot)   # for plotting multiple panels together via plot_grid function\nlibrary(ggeffects) # for extracting the marginal effects of each covariate in the model\nlibrary(MuMIn)   # for AIC\nlibrary(performance) # for R2\nlibrary(terra) #raster\nlibrary(tidyverse)\nlibrary(raster)\nlibrary(sf)\nlibrary(data.table)\nlibrary(mgcv)\n\n# Ensure the dataset has the proper format with predictors and the response variable (RealDets)\ndatextract &lt;- readRDS('data/datextract.RDS')\nalldat &lt;- readRDS('data/alldat.RDS')\n# As before, assemble our dataset remove NAs\ndatgam &lt;- cbind(datextract, alldat) %&gt;% drop_na() %&gt;% mutate(RealDets = as.factor(RealDets), Transmitter = as.factor(Transmitter))"
  },
  {
    "objectID": "GAM.html#run-gam-model",
    "href": "GAM.html#run-gam-model",
    "title": "RSF with GAMs",
    "section": "Run GAM model",
    "text": "Run GAM model\nWe will just use all of the data to run the GAM model in this example. GAMS do well with spatial and temporal autocorrelation!\n\n# Fit a GAM model with spatial and temporal autocorrelation (x, y, and Date). Stay tuned for \ngam_model &lt;- gam(RealDets ~ s(hw2020, k = 4) + s(tt2020, k = 4) + s(cov2020, k = 4) + s(sdcov2020, k = 4) + s(num2020, k = 4) +\n                   s(Transmitter, bs = \"re\") + # ID\n                   s(x, y, bs = \"gp\"), # Spatial component\n                 family = binomial, data = datgam, method = \"REML\")"
  },
  {
    "objectID": "GAM.html#summarizing-the-model",
    "href": "GAM.html#summarizing-the-model",
    "title": "RSF with GAMs",
    "section": "Summarizing the model",
    "text": "Summarizing the model\n\n# Summarize the model\nsummary(gam_model)\n## \n## Family: binomial \n## Link function: logit \n## \n## Formula:\n## RealDets ~ s(hw2020, k = 4) + s(tt2020, k = 4) + s(cov2020, k = 4) + \n##     s(Transmitter, bs = \"re\") + s(x, y, bs = \"gp\")\n## \n## Parametric coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -3.4897     0.5552  -6.286 3.26e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                   edf Ref.df Chi.sq  p-value    \n## s(hw2020)       2.939  2.997  42.41  &lt; 2e-16 ***\n## s(tt2020)       2.753  2.951  10.52   0.0226 *  \n## s(cov2020)      2.981  2.999 510.66  &lt; 2e-16 ***\n## s(Transmitter)  7.498 10.000  28.60 5.01e-05 ***\n## s(x,y)         31.585 31.934 811.59  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.636   Deviance explained = 57.7%\n## -REML = 1343.1  Scale est. = 1         n = 3948\n\n# Visualize smooth effects for each covariate\nplot(gam_model, pages = 1, all.terms = TRUE)\n\n\n\n\n\n\n\n\n# Predict probabilities for the full dataset and evaluate accuracy. Let's skip the test vs. training - it's similar to GLMMs. \ndatgam$predicted_probs_gam &lt;- predict(gam_model, newdata = datgam, type = \"response\")\ndatgam$predicted_class_gam &lt;- ifelse(datgam$predicted_probs_gam &gt; 0.5, 1, 0)"
  },
  {
    "objectID": "GAM.html#model-accuracy",
    "href": "GAM.html#model-accuracy",
    "title": "RSF with GAMs",
    "section": "Model accuracy",
    "text": "Model accuracy\nLet’s look at the model accuracy. You will see it is a lot lower than the RF model but better than the GLMM\n\n# Confusion matrix and accuracy for the GAM\nconf_matrix_gam &lt;- table(datgam$RealDets, datgam$predicted_class_gam)\nconf_matrix_gam\n##    \n##        0    1\n##   0 1779  247\n##   1  244 1678\naccuracy_gam &lt;- sum(diag(conf_matrix_gam)) / sum(conf_matrix_gam)\naccuracy_gam\n## [1] 0.8756332"
  },
  {
    "objectID": "GAM.html#model-interpretation",
    "href": "GAM.html#model-interpretation",
    "title": "RSF with GAMs",
    "section": "Model Interpretation",
    "text": "Model Interpretation\nLet’s map the predicted surface! Everything else is very similar to RF, GLMM\n\n# Predict probabilities on spatial grid using GAMs\ncov_2020 &lt;- rast('data/cov2020.tif') #percent SAV cover\nsdcov_2020 &lt;- rast('data/sdcov2020.tif') #standard deviation of cover\nnumsp_2020 &lt;- rast('data/num2020.tif') #number of SAV species\nhw_2020 &lt;- rast('data/hw2020.tif') #Halodule wrightii cover\ntt_2020 &lt;- rast('data/tt2020.tif')\n\nextent &lt;- st_read('data/trainr2021_mask.shp')\n## Reading layer `trainr2021_mask' from data source \n##   `C:\\Users\\jonro\\OneDrive\\Desktop\\RSF_OTN_Workshop\\RSF_OTN_Workshop\\data\\trainr2021_mask.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 1 feature and 1 field\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 518189.1 ymin: 2776305 xmax: 521241.5 ymax: 2780157\n## Projected CRS: NAD83 / UTM zone 17N\n\ncov2020 &lt;- terra::crop(cov_2020, extent)\nsdcov2020 &lt;- terra::crop(sdcov_2020, extent)\nnum2020 &lt;- terra::crop(numsp_2020, extent)\nhw2020 &lt;- terra::crop(hw_2020, extent)\ntt2020 &lt;- terra::crop(tt_2020, extent)\n\nrastdat &lt;- c(cov2020, sdcov2020, num2020, hw2020, tt2020)\nrastdat &lt;- terra::project(rastdat, 'epsg:2958')\n# Convert rasters into a data frame and predict onto spatial data\nnewdata_gam &lt;- raster::as.data.frame(rastdat, xy=T) %&gt;% \n  mutate(Transmitter = \"place-holder\")# Use rastdat from earlier\nnewdata_gam$predicted_probs_gam &lt;- predict(gam_model, newdata = newdata_gam, type = \"response\")\n## Warning in predict.gam(gam_model, newdata = newdata_gam, type = \"response\"):\n## factor levels place-holder not in original fit\n\n# Create a raster from predicted probabilities\npred_raster_gam &lt;- rast(ext(rastdat), resolution = res(rastdat), crs = crs(rastdat))\npred_raster_gam[] &lt;- newdata_gam$predicted_probs_gam\n\n# Plot the spatial predictions\nplot(pred_raster_gam, main = \"GAM Predicted Probabilities of Presence\")\n\n\n\n\n\n\n\n\n# Convert rasters to data frames\ndf_gam &lt;- as.data.frame(pred_raster_gam, xy = TRUE)\ncolnames(df_gam)[3] &lt;- \"GAM_Prob\" \n\n# Create the GAM plot\n(gam_plot &lt;- ggplot(df_gam, aes(x = x, y = y, fill = GAM_Prob)) +\n  geom_tile() +\n  scale_fill_viridis_c(limits = c(0, 1), name = \"Probability\") + \n  coord_equal() +\n  labs(title = \"GAM Predicted Probabilities of Presence\") +\n  theme_minimal())\n\n\n\n\n\n\n\n\nCongrats, you have used GAMs and now have gone through the entire workshop!"
  }
]